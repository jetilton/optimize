{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver for Total Dissolved Gas Prediction and Optimization\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Management and operation of dams within the Columbia River Basin (CRB) provides the region with irrigation, hydropower production, flood control, navigation, and fish passage. These various systemwide demands can require unique dam operations that may result in both voluntary and involuntary spill, thereby increasing tailrace levels of total dissolved gas (TDG) which can be fatal to fish. Appropriately managing TDG levels within the context of the systematic demands requires a predictive framework robust enough to capture the operationally related effects on TDG levels. \n",
    "\n",
    "The physical parameters such as spill and hydropower flow proportions, accompanied by the characteristics of the dam such as plant head levels and tailrace depths, are used to develop the empirically-based prediction model.\n",
    "\n",
    "### Required Technology\n",
    "\n",
    "This model was built using:\n",
    "- Python 3 (major packages):\n",
    "    - Papermill\n",
    "    - Scipy\n",
    "    - Numpy\n",
    "    - Pandas\n",
    "    - Yaml\n",
    "    - scikit-learn\n",
    "    \n",
    "- Nteract Jupyter Notebooks\n",
    "    - https://nteract.io/\n",
    "\n\nThe majority of the required packages are standard science/engineering python packages when using the Anaconda environment.  Papermill and Jupyter Nteract are a new technology.  Nteract allows Jupyter Notebooks to be parameterized and papermill allows you to access data saved within a single or group of executed notebooks to pass on to other notebooks or scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Organization\n",
    "\n",
    "This model consists of 3 parts\n",
    "1. Configuration file\n",
    "2. Script file\n",
    "3. Jupyter Notebooks (including this one)\n",
    "\n",
    "### Configuration File\n",
    "\n",
    "The configuration file is in yaml format that consists of the required project pathnames for the Coefficient optimization.  An example configuration file is seen below:\n",
    "\n\n",
    "the_dalles:<br>\n",
    "  &emsp;h_t: TDA.Elev-Tailwater.Ave.1Hour.1Hour.GDACS-COMPUTED-REV<br>\n",
    "  &emsp;p_atm: TDA.Pres-Air.Inst.1Hour.0.GOES-REV<br>\n",
    "  &emsp;q_p: TDA.Flow-Gen.Ave.1Hour.1Hour.CBT-REV<br>\n",
    "  &emsp;q_s: TDA.Flow-Spill.Ave.1Hour.1Hour.CBT-REV<br>\n",
    "  &emsp;temp_water: TDA.Temp-Water.Inst.1Hour.0.GOES-REV<br>\n",
    "  &emsp;tdg_f: TDA.%-Saturation-TDG.Inst.1Hour.0.GOES-COMPUTED-REV<br>\n",
    "  &emsp;tdg_tw: TDDO.%-Saturation-TDG.Inst.1Hour.0.GOES-COMPUTED-REV<br>\n",
    "\n\n",
    "### Script File\n",
    "\n",
    "The script util.py contains the 4 required equations (functions) found in the study \"Total dissolved gas prediction and optimization in RIVERWARE,\" as well as two additional functions for unit conversion.\n",
    "\n",
    "### Jupyter Notebooks and Model Flow\n",
    "\n",
    "<img src=\"../images/driver.png\">\n",
    "\n\n\nThe Image above walks through the linear model process and represents each Jupyter Notebook, with the arrow representing this Notebook as the driver of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks (let's drive!)\n",
    "\nWe will start with a parameterized cell for this notebook setting the config file and the project to run through the current optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "config_file = '../config.yml'\n",
    "project = 'the_dalles'\n",
    "lookback = 365*3\n",
    "DATA_DIR = '../projects/{}'\n",
    "maxiter = 2\n",
    "bootstraps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data_dir = DATA_DIR.format(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#creating directories to store model results\n",
    "\n",
    "import os\n",
    "\n",
    "directories = ['/error', '/results', '/train_test']\n",
    "for directory in directories:\n",
    "    os.mkdir(data_dir+directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n\n",
    "### Get Data\n",
    "\n",
    "Now let's get the data from the CWMS database.  I use a pre-canned function `get_cwms` that I created to extract data from CWMS and return a pandas dataframe.  This will be the 'raw' data for the project.  I say 'raw' because there is a process to interpolate that limits itself to 5 missing values.  Furthermore the data is converted into SI units because that is \n",
    "\n\n\n",
    "#### Parameters\n",
    "\n",
    " - project_dict\n",
    " - lookback\n",
    " \n",
    "#### Notebook\n",
    "\n",
    "[get_data.ipynb](get_data.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import papermill as pm\n",
    "config = yaml.load(open(config_file))\n",
    "project_dict = config[project]\n",
    "saved_notebook = data_dir + '/{}_raw.ipynb'.format(project)\n",
    "\n",
    "pm.execute_notebook(\n",
    "   'get_data.ipynb',\n",
    "   saved_notebook,\n",
    "   parameters = dict(project = project, project_dict=project_dict, lookback=lookback)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#get raw data to pass to split notebook\n",
    "nb = pm.read_notebook(saved_notebook)\n",
    "nb_df = nb.dataframe\n",
    "raw_data = nb_df[nb_df['name']=='raw_data']['value'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split â†’ Save Data\n",
    "I will now use two notebooks jointly, `split_data.ipynb` and `save_data.ipynb`.  This model will use a k-fold cross-validation with data split by year.  Each year will be a test dataset one time with the remaining years as the train set.  The hourly time series data is highly correlated in time so a random split would not produce an appropriate validation.  TDG offers a cyclic pattern where each year can be assumed independent of any other.  This will produce much better generalizations than a random split.  The split notebook feeds directly into the save notebook. So the data will be split and then saved in individual notebooks labeled by year that is the test set.\n",
    "\n\n",
    "#### Parameters\n",
    " - project \n",
    " - frequency\n",
    "\n",
    "#### Notebooks\n",
    " [split_data.ipynb](split_data.ipynb)\n",
    " \n",
    " [save_data.ipynb](save_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "raw_data = raw_data\n",
    "freq = 'Y' \n",
    "data_dir = data_dir\n",
    "project = project\n",
    "\n",
    "pm.execute_notebook(\n",
    "   'split_data.ipynb',\n",
    "   data_dir + '/{}_grouped.ipynb'.format(project),\n",
    "   parameters = dict(raw_data=raw_data, freq=freq, data_dir=data_dir, project=project)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "This is the heart of the model.  This will notebook optimizes the three coefficients *b1, b2, b3* by minimizing the root mean squared error.  The results of this model will feed into a monte carlo simulation of another model.  Therefore, I will create around 1000 models from the available data in a bootstrap process with each year providing 1000/n years models.  The distribution of coefficients will be used to construct the monte carlo model.  \n",
    "\n",
    "I will use the `minimize` function from [scipy's optimize] (https://docs.scipy.org/doc/scipy/reference/optimize.html) package.  It requires a first guess for the model coefficients.  I will optimize on all the data first as I think this will be a good first guess for each model.  Within year model bootstrap models will use an average of the previous model runs as a first guess.\n",
    " \n",
    "\n",
    "#### Parameters\n",
    " - data\n",
    " - x0\n",
    " - sample\n",
    " - maxiter\n",
    "\n",
    "The x0 parameter is the initial guess to start the optimization algorithm with.  The sample parameter is a boolean of whether to sample the data with replacement used for the bootstrap process.  The maxiter is the maximum number of iterations for the optimization algorithm before it breaks.\n",
    "\n\n",
    "#### Notebook\n",
    "[optimize.ipynb](optimize.ipynb)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters \n",
    "data = raw_data\n",
    "sample = False\n",
    "maxiter = maxiter\n",
    "x0 = [1,.5,150]\n",
    "\n",
    "optimized = data_dir + '/optimized.ipynb'\n",
    "\n",
    "pm.execute_notebook(\n",
    "           'optimize.ipynb',\n",
    "           optimized,\n",
    "           parameters = dict(data=raw_data, sample=False, maxiter=maxiter, x0=x0)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#get raw data to pass to split notebook and x0 to pass to error\n",
    "nb = pm.read_notebook(optimized)\n",
    "nb_df = nb.dataframe\n",
    "x = nb_df[nb_df['name']=='x']['value'].values[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error\n",
    "\n",
    "I will now calculate the error of the model by testing against the training data.\n",
    "\n",
    "#### Parameters\n",
    "    - b\n",
    "    - test_data\n",
    "#### Notebook\n",
    "[error.ipynb](error.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "test_data = raw_data\n",
    "b = x\n",
    "error = data_dir + '/errors.ipynb'\n",
    "\n",
    "pm.execute_notebook(\n",
    "   'error.ipynb',\n",
    "   error,\n",
    "   parameters = dict(test_data = test_data, b = b,)\n",
    "   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "I want to create a graphical representation on how well the model fits the actual data\n",
    "\n",
    "#### Parameters\n",
    "    - error_directory\n",
    "    - error_notebook\n",
    "#### Notebook\n",
    "[validation.ipynb](validation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "error_notebook = data_dir + '/errors.ipynb'\n",
    "validation = data_dir + '/validation.ipynb'\n",
    "pm.execute_notebook(\n",
    "   'validation.ipynb',\n",
    "   validation,\n",
    "   parameters = dict(error_notebook = error_notebook)\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "I will now run through the process again in a bootstrap to obtain a distribution of b coefficient values.  The bootstrap notebook runs through the optimization and error notebooks for each year.  A year is set as the test data and the otehr years are set as the train data.  Samples of the train data are optimized bootstraps/n year times to create a distribution of b coefficients to sample from in a future monte carlo model.\n",
    "\n",
    "### Parameters\n",
    "    - data_files\n",
    "    - maxiter\n",
    "    - bootstraps\n",
    "    - data_dir\n",
    "    - b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data_files = glob.glob(data_dir+'/train_test/*.ipynb')\n",
    "bootstrap_output = data_dir +'/bootstrap.ipynb'\n",
    "pm.execute_notebook(\n",
    "   'bootstrap.ipynb',\n",
    "   bootstrap_output,\n",
    "   parameters = dict(data_files=data_files, data_dir=data_dir, maxiter=maxiter, bootstraps=bootstraps, b=b)\n",
    "   )\n",
    "\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Run The validation notebook on all of the bootstrapped error notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "error_directory = data_dir + '/error'\n",
    "bootstrap_validation = data_dir + '/bootstrap_validation.ipynb'\n",
    "pm.execute_notebook(\n",
    "   'validation.ipynb',\n",
    "   bootstrap_validation,\n",
    "   parameters = dict(error_directory = error_directory)\n",
    "   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "nteract-on-jupyter@1.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
