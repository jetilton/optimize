{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "data_directory = os.path.abspath(os.path.join(r'D:\\gitClones\\nteract_models\\optimize\\projects'))\n",
    "if data_directory not in sys.path:\n",
    "    sys.path.append(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_dirs(data_directory, project_name):\n",
    "    directories = ['error', 'results', 'train_test']\n",
    "    project_directory = os.path.join(data_directory, project_name.replace(' ', '_').lower())\n",
    "    os.mkdir(project_directory)\n",
    "    for directory in directories:\n",
    "        os.mkdir(os.path.join(data_directory,project_name,directory)) \n",
    "    return project_directory\n",
    "\n",
    "def get_data(project_name, project_directory, project_dict, lookback):\n",
    "    name = '{}_{}.ipynb'.format(project_name,'raw')\n",
    "    raw_data_path = os.path.join(project_directory,name)\n",
    "    pm.execute_notebook(\n",
    "                   r'get_data.ipynb',\n",
    "                   raw_data_path,\n",
    "                   parameters = dict(project_dict=project_dict, lookback=lookback, project_name=project_name)\n",
    "                )\n",
    "    return raw_data_path\n",
    "    \n",
    "def get_raw_data(raw_data_path):\n",
    "    nb = pm.read_notebook(raw_data_path)\n",
    "    df = nb.dataframe\n",
    "    raw_data = df[df['name']=='raw_data']['value'].values[0]\n",
    "    return raw_data\n",
    " \n",
    "def split_data(raw_data, freq, project_directory, project_name):\n",
    "    #put data in data frame and set index\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "    df.set_index('index', inplace = True)\n",
    "    \n",
    "    #group data by given frequency\n",
    "    grouped = df.groupby(pd.Grouper(freq = freq))\n",
    "    \n",
    "    #create a dictionary of train/test sets by grouping frequency\n",
    "    grouped_dict = {}\n",
    "    for group, data in grouped:\n",
    "        train = df.drop(index = data.index)\n",
    "        test = data\n",
    "        train_test ={}\n",
    "        for d,name in zip([train, test], ['train', 'test']):\n",
    "            data ={}\n",
    "            for column in d.columns:\n",
    "                data.update({column:list(d[column].values)})\n",
    "            data.update({'index':[str(x) for x in d.index.values]})\n",
    "            train_test.update({name:data})\n",
    "        grouped_dict.update({group:train_test})\n",
    "    return grouped_dict\n",
    "\n",
    "def save_grouped_data(grouped_dict,project_name,project_directory):\n",
    "    #save the split data into individual notebooks\n",
    "    for k,v in grouped_dict.items():\n",
    "        file_name = '{}_{}'.format(project_name, str(k).replace(' ', '_').replace(':','_'))\n",
    "        relative_path = r'train_test\\{}.ipynb'.format(file_name)\n",
    "        abs_path = os.path.join(str(project_directory),relative_path)\n",
    "        pm.execute_notebook(\n",
    "           r'save_data.ipynb',\n",
    "           abs_path,\n",
    "           parameters = dict(group=str(k), data=v)\n",
    "        )\n",
    "\n",
    "def get_split_save_data(project_directory,data_directory,project_name, project_dict, lookback =700, freq = 'Y'):\n",
    "    raw_data_path = get_data(project_name, project_directory, project_dict, lookback)\n",
    "    raw_data = get_raw_data(raw_data_path)\n",
    "    grouped_dict = split_data(raw_data, freq, project_directory, project_name)\n",
    "    save_grouped_data(grouped_dict,project_name,project_directory)\n",
    "    return project_directory\n",
    "\n",
    "def optimize(file_name,train,test, maxiter = 3, x0 = [1,5,2],min_fb_tdg = float('-inf')):\n",
    "    pm.execute_notebook(\n",
    "           'optimize.ipynb',\n",
    "           file_name,\n",
    "           parameters = dict(train=train, test=test, maxiter=maxiter, x0=x0,min_fb_tdg=min_fb_tdg)\n",
    "        )\n",
    "       \n",
    "def optimize_split_data(project_directory, min_fb_tdg ='-inf', file_name_extension= '',maxiter = 3, x0 = [1,5,2]):\n",
    "    nbs = pm.read_notebooks(os.path.join(project_directory,r'train_test'))\n",
    "    df = nbs.dataframe\n",
    "    data = df[df['name']=='data']\n",
    "    grouped = data.groupby('filename')\n",
    "    i = 0\n",
    "    for g,v in grouped: \n",
    "        name = g.split('.')[0].replace('-','_')\n",
    "        train = v['value'].values[0]['train']\n",
    "        test = v['value'].values[0]['test']\n",
    "        file_name = os.path.join(project_directory,r'results/{}_optimized{}.ipynb'.format(name,file_name_extension))\n",
    "        \n",
    "        optimize(file_name,train=train, test=test, maxiter = maxiter, x0 = x0,min_fb_tdg = min_fb_tdg)\n",
    "        #get weighted average of x0 to start with for next batch\n",
    "        nb = pm.read_notebook(file_name)\n",
    "        nb_df = nb.dataframe\n",
    "        x = nb_df[nb_df['name']=='x']['value'].values[0]\n",
    "        x0 = list((np.array(x0) * i + np.array(x))/(i+1))\n",
    "        i+=1\n",
    "\n",
    "def optimize_raw_data(project_directory,project_name, min_fb_tdg ='-inf',maxiter = 3):\n",
    "    nbs = pm.read_notebooks(os.path.join(project_directory,r'results'))\n",
    "    df = nbs.dataframe\n",
    "    data = df[df['name']=='x']['value'].values\n",
    "    #use the mean coefficients from previous cross validated optimization to start\n",
    "    x0 = list(pd.DataFrame(columns = ['b0','b1','b2'], data = list(data)).mean().values)\n",
    "    raw_data_path = os.path.join(project_directory,'{}_{}'.format(project_name,'raw.ipynb'))\n",
    "    df = pm.read_notebook(raw_data_path).dataframe\n",
    "    raw_data = df[df['name']=='raw_data']['value'].values[0]\n",
    "    file_name_extension = '_all'\n",
    "    file_name = os.path.join(project_directory,r'results/{}_optimized{}.ipynb'.format(project_name,file_name_extension))\n",
    "    optimize(file_name,train=raw_data,test=raw_data, maxiter = maxiter, x0 = x0,min_fb_tdg = min_fb_tdg)\n",
    "\n",
    "def calculate_error(file_name, b, train, test, min_fb_tdg='-inf'):\n",
    "    pm.execute_notebook(\n",
    "           'error.ipynb',\n",
    "           file_name,\n",
    "           parameters = dict(file_name=file_name, train=train, test=test,min_fb_tdg=min_fb_tdg,b=b)\n",
    "        )\n",
    "    \n",
    "def run_error_on_split_data(project_directory):\n",
    "    results_directory = os.path.join(project_directory, 'results')\n",
    "    nbs = pm.read_notebooks(results_directory)\n",
    "    nb_df = nbs.dataframe\n",
    "    grouped = nb_df.groupby('filename')\n",
    "    for g,v in grouped:\n",
    "        name = g.split('.')[0]\n",
    "        train = v[v['name']=='train']['value'].values[0]\n",
    "        test =  v[v['name']=='test']['value'].values[0]\n",
    "        b = v[v['name']=='x']['value'].values[0]\n",
    "        min_fb_tdg = v[v['name']=='min_fb_tdg']['value'].values[0]\n",
    "        file_name = os.path.join(project_directory,'error/{}_error.ipynb'.format(name))\n",
    "        \n",
    "        calculate_error(file_name, b, train, test, min_fb_tdg=min_fb_tdg)\n",
    "\n",
    "    \n",
    "def run_validation(project_directory,project_name,file_extension=''):\n",
    "    error_directory = os.path.join(project_directory,'error')\n",
    "    file_name = '{}_{}{}{}'.format(project_name,'validation',file_extension,'.ipynb')\n",
    "    abs_path = os.path.join(project_directory,file_name)\n",
    "    pm.execute_notebook(\n",
    "           'validation.ipynb',\n",
    "           abs_path,\n",
    "           parameters = dict(error_directory=error_directory)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h_t': 'BON.Elev-Tailwater.Inst.1Hour.0.CBT-RAW',\n",
       " 'p_atm': 'BON.Pres-Air.Inst.1Hour.0.GOES-REV',\n",
       " 'q_p': 'BON.Flow-Gen.Ave.1Hour.1Hour.CBT-REV',\n",
       " 'q_s': 'BON.Flow-Spill.Ave.1Hour.1Hour.CBT-REV',\n",
       " 'tdg_f': 'BON.%-Saturation-TDG.Inst.1Hour.0.GOES-COMPUTED-REV',\n",
       " 'tdg_tw': 'CCIW.%-Saturation-TDG.Inst.1Hour.0.GOES-COMPUTED-REV',\n",
       " 'temp_water': 'BON.Temp-Water.Inst.1Hour.0.GOES-REV'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = yaml.load(open('../config.yml'))\n",
    "config.pop('bonneville',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'Y'\n",
    "lookback = 10000\n",
    "min_fb_tdg = '-inf'\n",
    "maxiter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  error.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\temp\\error/john_day_optimized_all_error.ipynb\n",
      "\n",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]\n",
      " 12%|██████████▌                                                                         | 1/8 [00:00<00:05,  1.17it/s]\n",
      " 25%|█████████████████████                                                               | 2/8 [00:01<00:05,  1.15it/s]\n",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:02<00:04,  1.14it/s]\n",
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:03<00:03,  1.15it/s]\n",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [00:27<00:23,  7.95s/it]\n",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [00:28<00:11,  5.80s/it]\n",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [00:30<00:04,  4.55s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:31<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_name = 'John Day'\n",
    "project_dict = v\n",
    "project_directory = os.path.join(data_directory, project_name.replace(' ', '_').lower())\n",
    "#calculate the error on each fold\n",
    "run_error_on_split_data(r'D:\\gitClones\\nteract_models\\optimize\\projects\\temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
