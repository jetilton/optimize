{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize\n",
    "This is the heart of the model.  This will notebook optimizes the three coefficients *b1, b2, b3* by minimizing the root mean squared error.  The results of this model will feed into a monte carlo simulation of another model.  Therefore, I will create around 1000 models from the available data in a bootstrap process with each year providing 1000/n years models.  The distribution of coefficients will be used to construct the monte carlo model.  \n",
    "\n",
    "I will use the `minimize` function from [scipy's optimize] (https://docs.scipy.org/doc/scipy/reference/optimize.html) package.  It requires a first guess for the model coefficients.  I will optimize on all the data first as I think this will be a good first guess for each model.  Within year model bootstrap models will use an average of the previous model runs as a first guess.\n",
    " \n",
    "\n",
    "#### Parameters\n",
    " - data\n",
    " - x0\n",
    " - sample\n",
    " - maxiter\n",
    " - min_fb_tdg\n",
    "\n",
    "The x0 parameter is the initial guess to start the optimization algorithm with.  The sample parameter is a boolean of whether to sample the data with replacement used for the bootstrap process.  The maxiter is the maximum number of iterations for the optimization algorithm before it breaks.\n",
    "\n",
    "\n",
    "#### Notebook\n",
    "[optimize.ipynb](optimize.ipynb)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "data_dir = r'D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles'\n",
    "maxiter = 100\n",
    "x0 = [1,.5,2]\n",
    "sample = False\n",
    "min_fb_tdg = 120\n",
    "file_name_extension = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input Notebook:  optimize.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles/results/the_dalles_2014-12-31_00_00_00_optimized.ipynb\n",
      "100%|██████████| 10/10 [00:24<00:00,  1.65s/it]\n",
      "Input Notebook:  optimize.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles/results/the_dalles_2015-12-31_00_00_00_optimized.ipynb\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.00it/s]\n",
      "Input Notebook:  optimize.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles/results/the_dalles_2016-12-31_00_00_00_optimized.ipynb\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.54it/s]\n",
      "Input Notebook:  optimize.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles/results/the_dalles_2017-12-31_00_00_00_optimized.ipynb\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.15it/s]\n",
      "Input Notebook:  optimize.ipynb\n",
      "Output Notebook: D:\\gitClones\\nteract_models\\optimize\\projects\\the_dalles/results/the_dalles_2018-12-31_00_00_00_optimized.ipynb\n",
      "100%|██████████| 10/10 [00:28<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "nbs = pm.read_notebooks(data_dir+'/train_test')\n",
    "df = nbs.dataframe\n",
    "data = df[df['name']=='data']\n",
    "grouped = data.groupby('filename')\n",
    "i = 0\n",
    "for g,v in grouped: \n",
    "    name = g.split('.')[0]\n",
    "    train_test_data = v['value'].values[0]\n",
    "    file_name = data_dir + '/results/{}_optimized{}.ipynb'.format(name,file_name_extension)\n",
    "    \n",
    "    pm.execute_notebook(\n",
    "               'optimize.ipynb',\n",
    "               file_name,\n",
    "               parameters = dict(data=train_test_data, sample=sample, maxiter=maxiter, x0=x0,min_fb_tdg=min_fb_tdg)\n",
    "            )\n",
    "    #get weighted average of x0 to start with for next batch\n",
    "    nb = pm.read_notebook(file_name)\n",
    "    nb_df = nb.dataframe\n",
    "    x = nb_df[nb_df['name']=='x']['value'].values[0]\n",
    "    x0 = list((np.array(x0) * i + np.array(x))/(i+1))\n",
    "    i+=1\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "nteract-on-jupyter@1.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
